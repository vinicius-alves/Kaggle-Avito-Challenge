{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento dos CSVs: 24.88s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtype={\n",
    "       \"region\": object, \n",
    "       \"city\":object, \n",
    "       \"parent_category_name\":object,\n",
    "       \"category_name\": object,\n",
    "       \"title\":object,\n",
    "       \"description\":object, \n",
    "       \"price\":np.float32, \n",
    "       \"activation_date\": object, \n",
    "       \"user_type\": object, \n",
    "       \"image\":object, \n",
    "       \"image_top_1\":np.float32, \n",
    "       \"deal_probability\":np.float32,\n",
    "       \"item_seq_number\": np.uint,\n",
    "       \"item_id\": object,\n",
    "       \"param_1\": object,\n",
    "       \"param_2\": object,\n",
    "       \"param_3\": object,\n",
    "       \"user_id\": object\n",
    "      }\n",
    "\n",
    "start = time.time()\n",
    "df_train = pd.read_csv('./data/train.csv', dtype = dtype, encoding='utf8')\n",
    "df_test  = pd.read_csv('./data/test.csv' , dtype = dtype, encoding='utf8')\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de carregamento dos CSVs: '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "\n",
    "text_columns        = [\"title\",\"description\"]\n",
    "colunas_categoricas_preenchidas = [\"user_type\",\"parent_category_name\",\"category_name\", \"user_id\",\"image_top_1\"]\n",
    "colunas_numericas_preenchidas   = [\"item_seq_number\"]\n",
    "\n",
    "'''\n",
    "As colunas categóricas parcialmente preenchidas são: param_1, param_2, param_3\n",
    "A coluna numérica parcialmente preenchida é price\n",
    "Logo só é necessário se preocupar em preencher-las\n",
    "'''\n",
    "\n",
    "\n",
    "matrizes_train_resultado = []\n",
    "matrizes_test_resultado = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminando colunas desnecessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop =['item_id']\n",
    "df_train.drop(columns=columns_to_drop, inplace=True)\n",
    "df_test.drop (columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apagando dados de treinamento não preenchidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace =True)\n",
    "#df_train = df_train.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregamento das features geradas pelas imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados faltantes de coluna width preenchidos com 360.0\n",
      "Dados faltantes de coluna height preenchidos com 360.0\n",
      "Dados faltantes de coluna size preenchidos com 41615.0\n",
      "Dados faltantes de coluna dullness preenchidos com 0.0\n",
      "Dados faltantes de coluna whiteness preenchidos com 0.0\n",
      "Dados faltantes de coluna average_red preenchidos com 0.4398931394263132\n",
      "Dados faltantes de coluna average_green preenchidos com 0.4695427332970225\n",
      "Dados faltantes de coluna average_blue preenchidos com 0.5056483297022513\n",
      "Dados faltantes de coluna average_pixel_width preenchidos com 2.6122685185185186\n",
      "Dados faltantes de coluna blurrness_score preenchidos com 554.7746435565629\n",
      "\n",
      "\n",
      "Tempo de merge com dataset de dados das imagens : 13.91s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Features calculadas em notebook externo\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# merge dados de treino\n",
    "#df_train[\"image\"].fillna(value=\"no-image\", inplace=True)\n",
    "df_images_train = pd.read_csv('./data/train_jpg.csv', encoding='utf8')\n",
    "df_train  = df_train.merge(df_train.merge(df_images_train, left_on = \"image\", right_on = \"id\", copy=False, sort=False),copy=False)\n",
    "df_images_train = None\n",
    "df_train.drop(columns=[\"image\",\"id\"],inplace=True)\n",
    "\n",
    "# merge dados de teste\n",
    "df_test[\"image\"].fillna(value=\"no-image\", inplace=True)\n",
    "df_images_test = pd.read_csv('./data/test_jpg.csv', encoding='utf8')\n",
    "df_test  = df_test.merge(df_test.merge(df_images_test, left_on = \"image\", right_on = \"id\", copy=False, sort=False),copy=False)\n",
    "df_test.drop(columns=[\"image\",\"id\"],inplace=True)\n",
    "\n",
    "df_images_test.drop(columns=[\"id\"], inplace = True)\n",
    "numeric_columns_images = df_images_test.columns.tolist()\n",
    "colunas_numericas_preenchidas += numeric_columns_images\n",
    "df_images_test = None\n",
    "\n",
    "# dados de teste sem imagens\n",
    "for column in numeric_columns_images:\n",
    "    serie_all_data = pd.concat([df_train[column],df_test[column]])\n",
    "    df_test[column].fillna(value=serie_all_data.median(), inplace=True)\n",
    "    #df_train[column].fillna(value=serie_all_data.median(), inplace=True)\n",
    "    print(\"Dados faltantes de coluna \"+ column +\" preenchidos com \"+str(serie_all_data.median()))\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "serie_all_data = None\n",
    "numeric_columns_images = None\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de merge com dataset de dados das imagens : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processando colunas geográficas   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de merge com dataset de dados geográficos : 11.40s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coordenadas das cidades fornecidas pelo usuário FrankHerfert\n",
    "# Ref.: https://www.kaggle.com/frankherfert/region-and-city-details-with-lat-lon-and-clusters/data\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Carregando dataset e fazendo merge\n",
    "df_coords = pd.read_csv('./data/avito_region_city_features.csv', encoding='utf8')\n",
    "df_coords.drop(columns=[\"region_id\",\"city_region_id\",\"city_region\"], inplace=True)\n",
    "df_train  = df_train.merge(df_train.merge(df_coords,on = [\"region\",\"city\"], copy=False, sort=False),copy=False)\n",
    "df_test   = df_test.merge(df_test.merge(df_coords,on = [\"region\",\"city\"], copy=False, sort=False),copy=False)\n",
    "\n",
    "# Apgando colunas de chaves\n",
    "df_train.drop (columns=[\"region\",\"city\"], inplace = True)\n",
    "df_test.drop  (columns=[\"region\",\"city\"], inplace = True)\n",
    "df_coords.drop(columns=[\"region\",\"city\"], inplace = True)\n",
    "\n",
    "colunas_numericas_preenchidas += df_coords.columns.tolist()\n",
    "\n",
    "# Liberando memória\n",
    "df_coords = None\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de merge com dataset de dados geográficos : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de colunas temporais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento da coluna de datas : 4.40s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convertendo datas\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "date_column_name = 'activation_date'\n",
    "df_train[date_column_name] = df_train[date_column_name].astype('datetime64[ns]', copy=False)\n",
    "df_test [date_column_name] = df_test[date_column_name].astype('datetime64[ns]', copy=False)\n",
    "\n",
    "# Data será transforamda em três colunas: ano - 1970, mês e dia\n",
    "df_train[date_column_name] = df_train[date_column_name].map(lambda t: [t.year -1970, t.month,t.day]) \n",
    "df_train['year']  = df_train[date_column_name].map(lambda t: t[0]) \n",
    "df_train['month'] = df_train[date_column_name].map(lambda t: t[1])\n",
    "df_train['day']   = df_train[date_column_name].map(lambda t: t[2])\n",
    "df_train.drop(columns=[date_column_name], inplace=True)\n",
    "\n",
    "df_test[date_column_name] = df_test[date_column_name].map(lambda t: [t.year -1970, t.month,t.day]) \n",
    "df_test['year']  = df_test[date_column_name].map(lambda t: t[0]) \n",
    "df_test['month'] = df_test[date_column_name].map(lambda t: t[1])\n",
    "df_test['day']   = df_test[date_column_name].map(lambda t: t[2])\n",
    "df_test.drop(columns=[date_column_name], inplace=True)\n",
    "\n",
    "numeric_columns_dates = [\"year\",\"month\",\"day\"]\n",
    "colunas_numericas_preenchidas += numeric_columns_dates\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de processamento da coluna de datas : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#df_train[numeric_columns_dates].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de texto corrido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento da coluna title : 154.87s\n",
      "\n",
      "Tempo de processamento da coluna description : 1007.78s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from IPython.core.display import HTML\n",
    "import string\n",
    "\n",
    "#exibir_dataframe = lambda dataframe: display(HTML(dataframe.head(5).to_html()))\n",
    "\n",
    "#Stop words russas\n",
    "stop_words_ru = open('data/stopwords.txt', encoding='utf8').read().split('\\n')\n",
    "stop_words_ru = np.array(list(map(lambda x: str.lower(x),stop_words_ru)))\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    stemmer = RussianStemmer()\n",
    "    analyzer = CountVectorizer().build_analyzer()\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "# Referência para pontuações\n",
    "dict_punctuation = {}\n",
    "for i in string.punctuation:\n",
    "    dict_punctuation[i] = ' '\n",
    "dict_punctuation = str.maketrans(dict_punctuation)\n",
    "\n",
    "#Pré processa os dados\n",
    "for column in text_columns:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Tratamento do texto\n",
    "    # passando tudo para minúsculo\n",
    "    df_train[column] = df_train[column].apply(lambda x: str.lower(str(x)))\n",
    "    df_test [column] = df_test [column].apply(lambda x: str.lower(str(x)))\n",
    "    # removendo pontuação\n",
    "    df_train[column] = df_train[column].apply(lambda text:text.translate(dict_punctuation))\n",
    "    df_test [column] = df_test [column].apply(lambda text:text.translate(dict_punctuation))\n",
    "    # removendo espaços\n",
    "    df_train[column] = df_train[column].apply(lambda x: str(x).strip())\n",
    "    df_test [column] = df_test [column].apply(lambda x: str(x).strip())\n",
    "    # removendo stop words\n",
    "    df_train[column] = df_train[column].apply(lambda x: ' '.join([word.strip() for word in x.split() if word.strip() not in stop_words_ru]))\n",
    "    df_test [column] = df_test [column].apply(lambda x: ' '.join([word.strip() for word in x.split() if word.strip() not in stop_words_ru]))\n",
    "  \n",
    "    # Criando tfIdfVectorizer\n",
    "    tfidf = TfidfVectorizer(sublinear_tf=True, analyzer=stemmed_words,\n",
    "                        lowercase=True, min_df = 2, max_df=0.8, dtype = np.float64)\n",
    "\n",
    "    # fit \n",
    "    tfidf.fit(df_train[column])\n",
    "\n",
    "    # transform\n",
    "    train_column_transformed = tfidf.transform(df_train[column])\n",
    "    test_column_transformed  = tfidf.transform(df_test[column])\n",
    "    \n",
    "    # apagando colunas antigas\n",
    "    df_train.drop(columns=[column],inplace=True)\n",
    "    df_test.drop (columns=[column],inplace=True)\n",
    "    \n",
    "    # Salvando matrizes de resultado\n",
    "    matrizes_train_resultado.append(train_column_transformed)\n",
    "    matrizes_test_resultado.append(test_column_transformed)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print('Tempo de processamento da coluna '+column+' : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#print(\"Coluna \"+column+ \" transformada em:\")\n",
    "#exibir_dataframe(pd.DataFrame(train_column_transformed[:3].toarray(), columns = tfidf.vocabulary_.keys()))\n",
    "\n",
    "# Limpando memória\n",
    "train_column_transformed = None\n",
    "test_column_transformed  = None\n",
    "tfidf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de colunas categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento da coluna user_type : 1.17s\n",
      "\n",
      "Tempo de processamento da coluna parent_category_name : 1.33s\n",
      "\n",
      "Tempo de processamento da coluna category_name : 1.49s\n",
      "\n",
      "Tempo de processamento da coluna user_id : 6.24s\n",
      "\n",
      "Tempo de processamento da coluna image_top_1 : 2.05s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def dropcols_coo(M, idx_to_drop):\n",
    "    # Apaga coluna idx_to_drop em matriz esparsa M\n",
    "    idx_to_drop = np.unique(idx_to_drop)\n",
    "    C = M.tocoo()\n",
    "    M = None\n",
    "    keep = ~np.in1d(C.col, idx_to_drop)\n",
    "    C.data, C.row, C.col = C.data[keep], C.row[keep], C.col[keep]\n",
    "    C.col -= idx_to_drop.searchsorted(C.col)    \n",
    "    C._shape = (C.shape[0], C.shape[1] - len(idx_to_drop))\n",
    "    return C.tocsr()\n",
    "\n",
    "num_linhas_treino = df_train.shape[0]\n",
    " \n",
    "def hotEncodar(column, apagar_coluna_antiga = True):\n",
    "    start = time.time()\n",
    "    \n",
    "    df_all_data = pd.DataFrame()\n",
    "    df_all_data[column] = pd.concat([df_train[column],df_test [column]],copy=False)    \n",
    "    df_all_data[column] = df_all_data[column].astype(str, copy = False)\n",
    "    \n",
    "     # fit labels\n",
    "    label_enc = preprocessing.LabelEncoder() \n",
    "    label_enc.fit(df_all_data[column])\n",
    "    \n",
    "    # transform labels\n",
    "    column_label_encoded = label_enc.transform(df_all_data[column]).reshape(-1,1) \n",
    "    df_all_data = None\n",
    "        \n",
    "    # fit e transform usando OneHotEncoder\n",
    "    one_hot_enc = OneHotEncoder(dtype=np.uint8, sparse=True)\n",
    "    column_transformed = one_hot_enc.fit_transform(column_label_encoded)\n",
    "    \n",
    "    # particionando resultado\n",
    "    train_column_transformed = column_transformed[:num_linhas_treino]\n",
    "    test_column_transformed  = column_transformed[num_linhas_treino:]\n",
    "    column_transformed = None\n",
    "    \n",
    "    # removendo coluna linearmente dependente do resultado\n",
    "    indice_ultima_coluna = train_column_transformed.shape[1]-1\n",
    "    train_column_transformed = dropcols_coo(train_column_transformed, indice_ultima_coluna) \n",
    "    indice_ultima_coluna = test_column_transformed.shape[1]-1\n",
    "    test_column_transformed = dropcols_coo(test_column_transformed, indice_ultima_coluna)\n",
    "    \n",
    "    # apagando colunas antigas\n",
    "    if(apagar_coluna_antiga):\n",
    "        df_train.drop(columns=[column], inplace=True)\n",
    "        df_test.drop(columns=[column], inplace=True)\n",
    "    \n",
    "    # Salvando matrizes de resultado\n",
    "    matrizes_train_resultado.append(train_column_transformed)\n",
    "    matrizes_test_resultado.append(test_column_transformed)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print('Tempo de processamento da coluna '+column+' : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "    \n",
    "for column in colunas_categoricas_preenchidas:\n",
    "    hotEncodar(column)\n",
    "\n",
    "# Limpando memória\n",
    "train_column_transformed = None\n",
    "test_column_transformed  = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento para normalização dos dados : 0.27s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_all_data = pd.concat([df_train[colunas_numericas_preenchidas],df_test[colunas_numericas_preenchidas]],copy=False)\n",
    "\n",
    "# Escalando colunas numéricas \n",
    "scaler = MinMaxScaler(copy=False)\n",
    "scaler.fit(df_all_data[colunas_numericas_preenchidas])\n",
    "train_matriz_scaled = scaler.transform(df_train[colunas_numericas_preenchidas])\n",
    "test_matriz_scaled  = scaler.transform(df_test[colunas_numericas_preenchidas])\n",
    "    \n",
    "# Salvando matrizes de resultado\n",
    "matrizes_train_resultado.append(train_matriz_scaled)\n",
    "matrizes_test_resultado.append(test_matriz_scaled)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de processamento para normalização dos dados : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#print(\"Coluna numéricas transformadas em:\")\n",
    "#exibir_dataframe(pd.DataFrame(train_matriz_scaled[:3], columns = numeric_columns))\n",
    "\n",
    "# liberando memória\n",
    "train_matriz_scaled = None\n",
    "test_matriz_scaled = None\n",
    "df_all_data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando param_1, param_2 e param_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "# Gerando matrizes\n",
    "matriz_train = sps.hstack(matrizes_train_resultado).tocsr()\n",
    "matriz_test  = sps.hstack(matrizes_test_resultado).tocsr()\n",
    "\n",
    "#df_test[['param_1','param_2','param_3']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preenchendo param_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento da coluna param_1 : 1.38s\n",
      "\n",
      "Tempo para preencher coluna param_1 : 4.23s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "indices_nan_test = np.array(df_test[\"param_1\"][df_test[\"param_1\"].isnull()].index)\n",
    "\n",
    "df_train[\"param_1\"] = df_train[\"param_1\"].map(lambda x : str(x))\n",
    "\n",
    "if(len(indices_nan_test)>0):\n",
    "\n",
    "    clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "    clf.fit(matriz_train, df_train[\"param_1\"])\n",
    "\n",
    "    classes_preditas = clf.predict(matriz_test[indices_nan_test])\n",
    "\n",
    "    df_test[\"param_1\"].update(pd.Series(classes_preditas, index=indices_nan_test))\n",
    "\n",
    "#hotEncoded\n",
    "hotEncodar(column = \"param_1\", apagar_coluna_antiga = False)\n",
    "\n",
    "# Atualizando matrizes\n",
    "matriz_train = sps.hstack(matrizes_train_resultado).tocsr()\n",
    "matriz_test  = sps.hstack(matrizes_test_resultado).tocsr()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo para preencher coluna param_1 : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#df_test[['param_1','param_2','param_3']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preenchendo param_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento da coluna param_2 : 1.37s\n",
      "\n",
      "Tempo para preencher coluna param_2 : 19.32s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "classes_param_1 = df_train['param_1'].unique()\n",
    "\n",
    "df_train[\"param_2\"] = df_train[\"param_2\"].map(lambda x : str(x))\n",
    "\n",
    "for class_param_1 in classes_param_1:\n",
    "    \n",
    "    indices_treinamento = df_train[\"param_1\"][df_train[\"param_1\"] == class_param_1].index\n",
    "    \n",
    "    df_train_class_param_1 = df_train[df_train[\"param_1\"] == class_param_1]\n",
    "    df_test_class_param_1  = df_test [df_test [\"param_1\"] == class_param_1]\n",
    "    \n",
    "    indices_nan_test_column = np.array(df_test_class_param_1[\"param_2\"][df_test_class_param_1[\"param_2\"].isnull()].index)\n",
    "    \n",
    "    if(len(indices_nan_test_column)>0):\n",
    "        \n",
    "        clf = RandomForestClassifier(n_jobs=-1)\n",
    "    \n",
    "        clf.fit(matriz_train[indices_treinamento], df_train_class_param_1[\"param_2\"])\n",
    "    \n",
    "        classes_preditas = clf.predict(matriz_test[indices_nan_test_column])\n",
    "\n",
    "        df_test[\"param_2\"].update(pd.Series(classes_preditas, index=indices_nan_test_column))\n",
    "        \n",
    "\n",
    "#hotEncoded\n",
    "hotEncodar(column = \"param_2\", apagar_coluna_antiga = False)\n",
    "\n",
    "# Atualizando matrizes\n",
    "matriz_train = sps.hstack(matrizes_train_resultado).tocsr()\n",
    "matriz_test  = sps.hstack(matrizes_test_resultado).tocsr()\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo para preencher coluna param_2 : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#df_test[['param_1','param_2','param_3']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preenchendo param_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento da coluna param_3 : 1.42s\n",
      "\n",
      "Tempo para preencher coluna param_3 : 27.55s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "classes_param_1 = df_train['param_2'].unique()\n",
    "\n",
    "df_train[\"param_3\"] = df_train[\"param_3\"].map(lambda x : str(x))\n",
    "\n",
    "for class_param_1 in classes_param_1:\n",
    "    \n",
    "    indices_treinamento = df_train[\"param_2\"][df_train[\"param_2\"] == class_param_1].index\n",
    "    \n",
    "    df_train_class_param_1 = df_train[df_train[\"param_2\"] == class_param_1]\n",
    "    df_test_class_param_1  = df_test [df_test [\"param_2\"] == class_param_1]\n",
    "    \n",
    "    indices_nan_test_column = np.array(df_test_class_param_1[\"param_3\"][df_test_class_param_1[\"param_3\"].isnull()].index)\n",
    "    \n",
    "    if(len(indices_nan_test_column)>0):\n",
    "        \n",
    "        clf = RandomForestClassifier(n_jobs=-1)\n",
    "    \n",
    "        clf.fit(matriz_train[indices_treinamento], df_train_class_param_1[\"param_3\"])\n",
    "    \n",
    "        classes_preditas = clf.predict(matriz_test[indices_nan_test_column])\n",
    "\n",
    "        df_test[\"param_3\"].update(pd.Series(classes_preditas, index=indices_nan_test_column))\n",
    "        \n",
    "        \n",
    "#hotEncoded\n",
    "hotEncodar(column = \"param_3\", apagar_coluna_antiga = False)\n",
    "\n",
    "# Atualizando matrizes\n",
    "matriz_train = sps.hstack(matrizes_train_resultado).tocsr()\n",
    "matriz_test  = sps.hstack(matrizes_test_resultado).tocsr()\n",
    "\n",
    "# Liberando memória\n",
    "matrizes_train_resultado = None\n",
    "matrizes_test_resultado  = None\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo para preencher coluna param_3 : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#df_test[['param_1','param_2','param_3']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     price\n",
       "0      NaN\n",
       "1   3000.0\n",
       "2  15000.0\n",
       "3   4500.0\n",
       "4   4900.0\n",
       "5    500.0\n",
       "6  20990.0\n",
       "7    990.0\n",
       "8   1200.0\n",
       "9    400.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test[['price']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preenchendo price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo para preencher coluna price : 0.68s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "indices_nan_test = np.array(df_test[\"price\"][df_test[\"price\"].isnull()].index)\n",
    "\n",
    "if(len(indices_nan_test)>0):\n",
    "\n",
    "    regr = Ridge(alpha=30)\n",
    "\n",
    "    regr.fit(matriz_train, np.log(df_train[\"price\"]+0.001))\n",
    "\n",
    "    ypred = regr.predict(matriz_test[indices_nan_test])\n",
    "\n",
    "    df_test[\"price\"].update(pd.Series(np.exp(ypred), index=indices_nan_test))\n",
    "\n",
    "#Liberando memória\n",
    "df_train = None\n",
    "matriz_train = None\n",
    "matriz_test = None\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo para preencher coluna price : '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "\n",
    "#df_test[['price']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando dados de teste preenchidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo para salvamento dos resultados : 20.54s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df_test_antigo = pd.read_csv('./data/test.csv' , dtype = dtype, encoding='utf8').head(df_test.shape[0])\n",
    "\n",
    "df_test_antigo[\"param_1\"] = df_test[\"param_1\"]\n",
    "df_test_antigo[\"param_2\"] = df_test[\"param_2\"]\n",
    "df_test_antigo[\"param_3\"] = df_test[\"param_3\"]\n",
    "df_test_antigo[\"price\"]   = df_test[\"price\"]\n",
    "\n",
    "df_test_antigo.to_csv('./data/test_preenchido.csv', encoding='utf-8', index=False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo para salvamento dos resultados : '+str(\"%.2f\" % (end - start))+'s\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
