{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando imports de todos os módulos que seram necessários para o processamento dos regressores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.sparse as sps\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados de validação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após realizar os imports temos que carregar os dados para o processamento em cada regressor, logo criamos três variáveis, para que possamos reaproveitar o carregamento dos dados em todos os regressorer. O dados que estão sendo carregados, são amostras retiradas da base de dados total, onde essas amostras tem o intuito de facilitar os testes de processamento de cada regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de carregamento das Matrizes: 0.13s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vetor_train_target = sps.load_npz('data/sparse_matrix_train_target_small.npz').tocsr()\n",
    "matriz_train       = sps.load_npz('data/sparse_matrix_train_small.npz').tocsr()\n",
    "matriz_test        = sps.load_npz('data/sparse_matrix_test_small.npz' ).tocsr()\n",
    "results            = []\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de carregamento das Matrizes: '+str(\"%.2f\" % (end - start))+'s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração do k-fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Fold é um método de validação cruzada, que consiste em dividir o conjunto total de dados em k subconjuntos mutuamente exclusivos do mesmo tamanho e, a partir disto, um subconjunto é utilizado para teste. Abaixo realizamos as configurações de alguns parâmetros que ele possui, como o número de divisões por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizando intervalo 7:4637 do conjunto de treino para validação, 4630 linhas.\n",
      "K-Fold utilizando 10 partições de 463 linhas do conjunto de treino.\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(0) - random_state\n",
    "\n",
    "# A validação cruzada é feita através de análise de partições pequenas\n",
    "# e mutuamente exclusivas dos datasets originais\n",
    "\n",
    "# Quantidade de divisões a serem usadas pelo k-fold\n",
    "n_splits = 10\n",
    "\n",
    "# Número de linhas utilizadas em cada divisão\n",
    "max_split_size = 1000\n",
    "\n",
    "# Obtendo conjunto de dados menor, através da escolha anterior\n",
    "num_linhas_treino = matriz_train.shape[0]\n",
    "\n",
    "if(max_split_size*n_splits > num_linhas_treino):\n",
    "    max_split_size = int(np.floor(num_linhas_treino/n_splits))\n",
    "\n",
    "inicio_faixa = np.random.randint(0,num_linhas_treino-max_split_size*n_splits+1)\n",
    "fim_faixa  = inicio_faixa + max_split_size*n_splits\n",
    "matriz_train = matriz_train[inicio_faixa:fim_faixa]\n",
    "vetor_train_target = vetor_train_target[inicio_faixa:fim_faixa]\n",
    "\n",
    "print(\"Utilizando intervalo \"+str(inicio_faixa)+\":\"+str(fim_faixa)+\" do conjunto de treino para validação, \"+str(fim_faixa-inicio_faixa)+\" linhas.\")\n",
    "print(\"K-Fold utilizando \"+str(n_splits)+\" partições de \"+str(max_split_size)+\" linhas do conjunto de treino.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliando Regressores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a etapa de pré-processamento de dados, começamos a avaliar o desempenho de alguns regressores em busca de encontrar os que consigam melhores resultados, a partir das matrizes geradas dos dados do conjunto de treino e do conjunto de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando regressor Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor linear: 6.56s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2597\n",
      "RMSE STD:   0.0091 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "start = time.time()\n",
    "\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "\n",
    "kf = KFold(n_splits=n_splits);\n",
    "\n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "        \n",
    "    # Criando Regressor Linear\n",
    "    regr = LinearRegression(n_jobs = -1)\n",
    "    \n",
    "    # Treino\n",
    "    regr.fit(X_train, y_train.toarray().ravel()) \n",
    "    \n",
    "    # Predição     \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "results.append((\"%.4f\" % np.mean(rmse_list)))\n",
    "print('Tempo de processamento do regressor linear: '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando regressor Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor ridge: 2.03s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2396\n",
      "RMSE STD:   0.0080 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "start = time.time()\n",
    "\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=n_splits);\n",
    " \n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "    # Criando Regressor Linear\n",
    "    regr = Ridge(alpha = 30.0, fit_intercept=True, normalize=False,\n",
    "                copy_X=True,max_iter=None,tol=0.001)\n",
    "\n",
    "    # Treino\n",
    "    regr.fit(X_train, y_train.toarray().ravel()) \n",
    "\n",
    "    # Predição     \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "results.append((\"%.4f\" % np.mean(rmse_list)))\n",
    "print('Tempo de processamento do regressor ridge: '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando regressor GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do GradientBoostingRegressor : 204.25s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2497\n",
      "RMSE STD:   0.0082 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "start = time.time()\n",
    "\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=n_splits);\n",
    "    \n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "    # Criando Regressor Linear\n",
    "    regr = GradientBoostingRegressor(learning_rate = 0.01)\n",
    "\n",
    "    # Treino\n",
    "    regr.fit(X_train, y_train.toarray().ravel()) \n",
    "\n",
    "    # Predição     \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "results.append((\"%.4f\" % np.mean(rmse_list)))\n",
    "print('Tempo de processamento do GradientBoostingRegressor : '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando regressor SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor SVM: 83.03s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2561\n",
      "RMSE STD:   0.0076 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "start = time.time()\n",
    "\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=n_splits);\n",
    "    \n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test];\n",
    "    y_train, y_test = y[train], y[test];\n",
    "\n",
    "    # Criando Regressor SVM\n",
    "    regr = SVR('linear')\n",
    "    \n",
    "    # Treino\n",
    "    regr.fit(X_train, y_train.toarray().ravel()) \n",
    "\n",
    "    # Predição     \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "results.append((\"%.4f\" % np.mean(rmse_list)))\n",
    "print('Tempo de processamento do regressor SVM: '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando regresor KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor KNN: 2.66s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2427\n",
      "RMSE STD:   0.0098 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "start = time.time()\n",
    "\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=n_splits);\n",
    "    \n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test];\n",
    "    y_train, y_test = y[train], y[test];\n",
    "\n",
    "    # Criando Regressor KNN\n",
    "    regr = KNeighborsRegressor(algorithm='auto', leaf_size=50, metric='minkowski',\n",
    "                               metric_params=None, n_jobs=1, n_neighbors=30, p=2,\n",
    "                               weights='distance')\n",
    "    \n",
    "    # Treino\n",
    "    regr.fit(X_train, y_train.toarray().ravel()) \n",
    "\n",
    "    # Predição     \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "results.append((\"%.4f\" % np.mean(rmse_list)))\n",
    "print('Tempo de processamento do regressor KNN: '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando regressor LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor LGBM: 4.69s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2515\n",
      "RMSE STD:   0.0082 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "start = time.time()\n",
    "\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=n_splits);\n",
    "    \n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test];\n",
    "    y_train, y_test = y[train], y[test];\n",
    "\n",
    "    # Criando Regressor LGBM\n",
    "    regr = LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "                         learning_rate=0.05, max_depth=-1, min_child_samples=20,\n",
    "                         min_child_weight=0.001, min_split_gain=0.0, n_estimators=15,\n",
    "                         n_jobs=-1, num_leaves=5, objective='regression', random_state=None,\n",
    "                         reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
    "                         subsample_for_bin=200000, subsample_freq=1) \n",
    "    \n",
    "    # Treino\n",
    "    regr.fit(X_train, y_train.toarray().ravel(),  eval_metric='metric') \n",
    "\n",
    "    # Predição     \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "   #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "results.append((\"%.4f\" % np.mean(rmse_list)))\n",
    "print('Tempo de processamento do regressor LGBM: '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando regressor XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor linear: 17.88s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2469\n",
      "RMSE STD:   0.0078 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "start = time.time()\n",
    "\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=n_splits);\n",
    "    \n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test];\n",
    "    y_train, y_test = y[train], y[test];\n",
    "        \n",
    "    # Criando Regressor Linear\n",
    "    regr = XGBRegressor(n_estimators=30)\n",
    "    \n",
    "    # Treino\n",
    "    regr.fit(X_train, y_train.toarray().ravel()) \n",
    "    \n",
    "    # Predição     \n",
    "    y_pred = regr.predict(X_test)\n",
    "    \n",
    "    #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "results.append((\"%.4f\" % np.mean(rmse_list)))\n",
    "print('Tempo de processamento do regressor linear: '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coparando os resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para faciliar a visualização dos resultados e a comparação dos regressores, o gráfico abaixo foi gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = list(np.float_(results))\n",
    "data = pd.DataFrame({'RMSE': results, \n",
    "                     'Regressores': ['Linear', 'Ridge', 'GradientBoosting', 'SVM', 'KNN', 'LGBM', 'XGBoost']})\n",
    "data.sort_values(by='RMSE', inplace=True);\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.barh(range(data.RMSE.count()), data.RMSE)\n",
    "\n",
    "plt.title(u\"Comparativo de RMSE de cada regressor\")\n",
    "plt.xlabel(\"RMSE\")\n",
    "plt.ylabel('Regressores')\n",
    "plt.xticks(data.RMSE)\n",
    "plt.yticks(range(len(results)), data.Regressores)\n",
    "plt.xlim(min(results) - 0.003, max(results) + 0.003)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinando Regressores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em busca da melhoria dos resultados obtidos, decidimos fazer uso de Ensemblers, que são métodos que combinam previsões de diferentes modelos para a geração de uma previsão final de melhor desempenho. Fizemos neste trabalho, testes com Bagging, AdaBoosting e Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5}: 1.39s\n",
      "\n",
      "RMSE MEAN:  0.2484\n",
      "RMSE STD:   0.0008\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 1.0}: 1.13s\n",
      "\n",
      "RMSE MEAN:  0.2451\n",
      "RMSE STD:   0.0015\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 1.0, 'max_samples': 0.5}: 1.74s\n",
      "\n",
      "RMSE MEAN:  0.2440\n",
      "RMSE STD:   0.0007\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 1.0, 'max_samples': 1.0}: 2.06s\n",
      "\n",
      "RMSE MEAN:  0.2426\n",
      "RMSE STD:   0.0012\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.5}: 1.25s\n",
      "\n",
      "RMSE MEAN:  0.2463\n",
      "RMSE STD:   0.0005\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 1.0}: 0.98s\n",
      "\n",
      "RMSE MEAN:  0.2440\n",
      "RMSE STD:   0.0007\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5}: 1.47s\n",
      "\n",
      "RMSE MEAN:  0.2434\n",
      "RMSE STD:   0.0009\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': True, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0}: 2.31s\n",
      "\n",
      "RMSE MEAN:  0.2421\n",
      "RMSE STD:   0.0008\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5}: 1.47s\n",
      "\n",
      "RMSE MEAN:  0.2478\n",
      "RMSE STD:   0.0007\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 1.0}: 0.87s\n",
      "\n",
      "RMSE MEAN:  0.2443\n",
      "RMSE STD:   0.0008\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': True, 'max_features': 1.0, 'max_samples': 0.5}: 1.78s\n",
      "\n",
      "RMSE MEAN:  0.2438\n",
      "RMSE STD:   0.0009\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': True, 'max_features': 1.0, 'max_samples': 1.0}: 1.09s\n",
      "\n",
      "RMSE MEAN:  0.2422\n",
      "RMSE STD:   0.0007\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 0.5}: 1.03s\n",
      "\n",
      "RMSE MEAN:  0.2462\n",
      "RMSE STD:   0.0002\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': False, 'max_features': 0.5, 'max_samples': 1.0}: 0.95s\n",
      "\n",
      "RMSE MEAN:  0.2441\n",
      "RMSE STD:   0.0004\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5}: 0.86s\n",
      "\n",
      "RMSE MEAN:  0.2435\n",
      "RMSE STD:   0.0008\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'bootstrap': False, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 1.0}: 0.87s\n",
      "\n",
      "RMSE MEAN:  0.2419\n",
      "RMSE STD:   0.0008\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False) params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 0.5}: 25.15s\n",
      "\n",
      "RMSE MEAN:  0.2565\n",
      "RMSE STD:   0.0008\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False) params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 0.5, 'max_samples': 1.0}: 39.06s\n",
      "\n",
      "RMSE MEAN:  0.2543\n",
      "RMSE STD:   0.0002\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Bagging - Estimador: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False) params: {'bootstrap': True, 'bootstrap_features': True, 'max_features': 1.0, 'max_samples': 0.5}: 51.13s\n",
      "\n",
      "RMSE MEAN:  0.2546\n",
      "RMSE STD:   0.0008\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d4efe965bfbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Predição\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    373\u001b[0m                 \u001b[0mtotal_n_estimators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 verbose=self.verbose)\n\u001b[0;32m--> 375\u001b[0;31m             for i in range(n_jobs))\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;31m# Reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[0;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_indices_mask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Draw samples, using a mask, and then fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[0;32m-> 1034\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX_csc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m                 tree.fit(X_csc, residual, sample_weight=sample_weight,\n\u001b[0;32m--> 785\u001b[0;31m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "#from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#randomState = check_random_state(0)\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=2);\n",
    "\n",
    "\n",
    "# Parametros a serem variados no bagging\n",
    "grid = ParameterGrid({\"max_samples\": [0.5, 1.0],\n",
    "                      \"max_features\": [0.5, 1.0],\n",
    "                      \"bootstrap\": [True, False],\n",
    "                      \"bootstrap_features\": [True, False]})\n",
    "\n",
    "# Testando cada estimador com cada configuração de parametros\n",
    "\n",
    "for base_estimator in [ \n",
    "                        Ridge(alpha = 29.0),\n",
    "                        GradientBoostingRegressor(learning_rate = 0.01),\n",
    "                        DummyRegressor(),\n",
    "                        DecisionTreeRegressor(),\n",
    "                        SVR(),\n",
    "                        KNeighborsRegressor(),\n",
    "                        #LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05, n_estimators=15),\n",
    "                        XGBRegressor()]:\n",
    "    for params in grid:\n",
    "        start = time.time()\n",
    "        \n",
    "        rmse_list =[]\n",
    "\n",
    "        for train, test in kf.split(X):\n",
    "            X_train, X_test = X[train], X[test];\n",
    "            y_train, y_test = y[train], y[test];\n",
    "            \n",
    "            # Criando Regressor Bagging\n",
    "            regr = BaggingRegressor(base_estimator=base_estimator,\n",
    "                                 **params)\n",
    "\n",
    "            # Treino\n",
    "            regr.fit(X_train, y_train.toarray().ravel()) \n",
    "\n",
    "            # Predição     \n",
    "            y_pred = regr.predict(X_test)\n",
    "\n",
    "            #Root mean squared error\n",
    "            rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "            end = time.time()\n",
    "\n",
    "        print('Tempo de processamento do regressor Bagging - Estimador: ' + str(base_estimator) + ' params: ' + str(params) + ': '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "        print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "        print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utlizando AdaBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor Adaboost - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'loss': 'linear'}: 5.18s\n",
      "\n",
      "RMSE MEAN:  0.2532\n",
      "RMSE STD:   0.0041\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'loss': 'square'}: 19.94s\n",
      "\n",
      "RMSE MEAN:  0.3031\n",
      "RMSE STD:   0.0024\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: Ridge(alpha=29.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001) params: {'loss': 'exponential'}: 22.50s\n",
      "\n",
      "RMSE MEAN:  0.2843\n",
      "RMSE STD:   0.0017\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False) params: {'loss': 'linear'}: 39.99s\n",
      "\n",
      "RMSE MEAN:  0.2530\n",
      "RMSE STD:   0.0002\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False) params: {'loss': 'square'}: 69.72s\n",
      "\n",
      "RMSE MEAN:  0.3210\n",
      "RMSE STD:   0.0041\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.01, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=100, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False) params: {'loss': 'exponential'}: 193.68s\n",
      "\n",
      "RMSE MEAN:  0.2972\n",
      "RMSE STD:   0.0033\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: DummyRegressor(constant=None, quantile=None, strategy='mean') params: {'loss': 'linear'}: 0.04s\n",
      "\n",
      "RMSE MEAN:  0.2652\n",
      "RMSE STD:   0.0000\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: DummyRegressor(constant=None, quantile=None, strategy='mean') params: {'loss': 'square'}: 0.03s\n",
      "\n",
      "RMSE MEAN:  0.2651\n",
      "RMSE STD:   0.0000\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: DummyRegressor(constant=None, quantile=None, strategy='mean') params: {'loss': 'exponential'}: 0.25s\n",
      "\n",
      "RMSE MEAN:  0.3167\n",
      "RMSE STD:   0.0000\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') params: {'loss': 'linear'}: 54.09s\n",
      "\n",
      "RMSE MEAN:  0.2764\n",
      "RMSE STD:   0.0002\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') params: {'loss': 'square'}: 50.32s\n",
      "\n",
      "RMSE MEAN:  0.2857\n",
      "RMSE STD:   0.0020\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=None, splitter='best') params: {'loss': 'exponential'}: 62.07s\n",
      "\n",
      "RMSE MEAN:  0.2776\n",
      "RMSE STD:   0.0015\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) params: {'loss': 'linear'}: 28.12s\n",
      "\n",
      "RMSE MEAN:  0.2673\n",
      "RMSE STD:   0.0003\n",
      "\n",
      "\n",
      "Tempo de processamento do regressor Adaboost - Estimador: SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) params: {'loss': 'square'}: 305.63s\n",
      "\n",
      "RMSE MEAN:  0.2617\n",
      "RMSE STD:   0.0055\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e31435497122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Predição\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m#Root mean squared error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_median_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m_get_median_predict\u001b[0;34m(self, X, limit)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;31m# Evaluate predictions of all estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         predictions = np.array([\n\u001b[0;32m-> 1061\u001b[0;31m             est.predict(X) for est in self.estimators_[:limit]]).T\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;31m# Sort the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;31m# Evaluate predictions of all estimators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m         predictions = np.array([\n\u001b[0;32m-> 1061\u001b[0;31m             est.predict(X) for est in self.estimators_[:limit]]).T\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;31m# Sort the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_sparse_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_support_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m             self.probA_, self.probB_)\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#randomState = check_random_state(0)\n",
    "rmse_list =[]\n",
    "X = matriz_train\n",
    "y = vetor_train_target\n",
    "kf = KFold(n_splits=2);\n",
    "\n",
    "\n",
    "# Parametros a serem variados no bagging\n",
    "grid = ParameterGrid({\"loss\": ['linear', 'square', 'exponential']})\n",
    "\n",
    "# Testando cada estimador com cada configuração de parametros\n",
    "for base_estimator in [ \n",
    "                        Ridge(alpha = 29.0),\n",
    "                        GradientBoostingRegressor(learning_rate = 0.01),\n",
    "                        DummyRegressor(),\n",
    "                        DecisionTreeRegressor(),\n",
    "                        SVR(),\n",
    "                        KNeighborsRegressor(),\n",
    "                        LGBMRegressor(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
    "                                             learning_rate=0.05, max_depth=-1, min_child_samples=20,\n",
    "                                             min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,\n",
    "                                             n_jobs=-1, num_leaves=200, objective='regression', random_state=None,\n",
    "                                             reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
    "                                             subsample_for_bin=200000, subsample_freq=1, task='train'),\n",
    "                        XGBRegressor()]:\n",
    "    for params in grid:\n",
    "        start = time.time()\n",
    "        \n",
    "        rmse_list =[]\n",
    "\n",
    "        for train, test in kf.split(X):\n",
    "            X_train, X_test = X[train], X[test];\n",
    "            y_train, y_test = y[train], y[test];\n",
    "            \n",
    "            # Criando AdaBoostRegressor\n",
    "            regr = AdaBoostRegressor(base_estimator=base_estimator,\n",
    "                                 **params)\n",
    "\n",
    "            # Treino\n",
    "            regr.fit(X_train, y_train.toarray().ravel()) \n",
    "\n",
    "            # Predição     \n",
    "            y_pred = regr.predict(X_test)\n",
    "\n",
    "            #Root mean squared error\n",
    "            rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "            end = time.time()\n",
    "\n",
    "        print('Tempo de processamento do regressor Adaboost - Estimador: ' + str(base_estimator) + ' params: ' + str(params) + ': '+str(\"%.2f\" % (end - start))+'s\\n')\n",
    "        print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "        print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de processamento do regressor linear: 36.22s\n",
      "\n",
      "\n",
      "RMSE MEAN:  0.2418\n",
      "RMSE STD:   0.0012 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rmse_list =[]\n",
    "X= matriz_train\n",
    "y= vetor_train_target\n",
    "X_test  = matriz_test\n",
    "kf = KFold(n_splits=2);\n",
    "\n",
    "# Definindo estimadores da primeira camada\n",
    "estimators_L1 = [\n",
    "    ( ExtraTreesRegressor(random_state=0, n_jobs=-1, \n",
    "                               n_estimators=40, max_depth=3)),\n",
    "    (BaggingRegressor(LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "                     learning_rate=0.05, max_depth=-1, min_child_samples=20,\n",
    "                     min_child_weight=0.001, min_split_gain=0.0, n_estimators=25,\n",
    "                     n_jobs=-1, num_leaves=270, objective='regression', random_state=None,\n",
    "                     reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
    "                     subsample_for_bin=200000, subsample_freq=1, task='train'))),\n",
    "    (RidgeCV(alphas = [0.1, 1.0, 10.0])),\n",
    "    (XGBRegressor()),\n",
    "    (BaggingRegressor(Ridge(alpha = 29.0,random_state=5), bootstrap = False, bootstrap_features= True, max_features = 1.0, max_samples = 1.0, n_jobs=-1)), \n",
    "    (BaggingRegressor(KNeighborsRegressor(30, 'distance', 'auto', 50), \n",
    "                           bootstrap = False, bootstrap_features= False, max_features = 1.0, max_samples = 1.0, n_jobs=-1, random_state=0))\n",
    "]\n",
    "\n",
    "# Definindo estimador final\n",
    "final_estimator = BaggingRegressor(( ExtraTreesRegressor(random_state=0, n_jobs=-1, \n",
    "                               n_estimators=30, max_depth=2)), bootstrap = False,\n",
    "                                   bootstrap_features= True, max_features = 1.0, max_samples = 0.5, n_jobs=-1,random_state=0)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# Validação cruzada    \n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test = X[train], X[test];\n",
    "    y_train, y_test = y[train], y[test];\n",
    "\n",
    "    # Criando Stacking Regressor \n",
    "    stregr = StackingRegressor(regressors=estimators_L1, \n",
    "                           meta_regressor=final_estimator)\n",
    "\n",
    "    # Treino\n",
    "    stregr = StackingRegressor(regressors=estimators_L1, \n",
    "                           meta_regressor=final_estimator)\n",
    "\n",
    "    stregr.fit(X_train, y_train.toarray().ravel()) \n",
    "    \n",
    "    # Predição     \n",
    "    y_pred = stregr.predict(X_test)\n",
    "    \n",
    "    #Root mean squared error\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(y_test.todense(), y_pred)));\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de processamento do regressor linear: '+str(\"%.2f\" % (end - start))+'s\\n\\n')\n",
    "print(\"RMSE MEAN: \",\"%.4f\" % np.mean(rmse_list))\n",
    "print(\"RMSE STD:  \",\"%.4f\" % np.std(rmse_list), '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados completos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o processamento de cada regressor com as amostras dadas, podemos realizar o processamento com o regressor que deu uma precisão melhor com todos os dados, logo abaixo temos 3 variáveis para poder carregar todos os dados disponíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "vetor_train_target = sps.load_npz('data/sparse_matrix_train_target.npz').tocsr()\n",
    "matriz_train       = sps.load_npz('data/sparse_matrix_train.npz').tocsr()\n",
    "matriz_test        = sps.load_npz('data/sparse_matrix_test.npz' ).tocsr()\n",
    "end = time.time()\n",
    "\n",
    "print('Tempo de carregamento das Matrizes: '+str(\"%.2f\" % (end - start))+'s\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando regressor em dados de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para gerar os resultados utilizamos o regressor que deu a melhor precisão nas avaliações das amostras abaixo e submetemos todos os dados disponíveis a ele, para que ele possar gerar o resultado esperado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = matriz_train\n",
    "y_train = vetor_train_target\n",
    "X_test  = matriz_test\n",
    "\n",
    "# Criando Regressor Ridge\n",
    "regr = Ridge(alpha = 30.0, fit_intercept=True, normalize=False,\n",
    "                copy_X=True,max_iter=None,tol=0.001)\n",
    "\n",
    "# Treino\n",
    "regr.fit(X_train, y_train.toarray().ravel()) \n",
    "\n",
    "# Predição      \n",
    "y_pred = regr.predict(X_test)\n",
    "\n",
    "# Melhorando resultado\n",
    "def fronteira(y):\n",
    "    if y >1:\n",
    "        return 1.0\n",
    "    elif y <0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return y\n",
    "    \n",
    "y_pred = np.array(list(map(fronteira, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando o resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a fase de avaliação dos regressores e a fase de gerar os resultados, criamos um arquivo com a extenção .csv, onde nesse arquivo tem todos os produtos com sua deal_probability anexada a cada produto. No campo deal_probability estão os resultados alcançados pelo processamento realizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_item_id  = pd.read_csv('./data/test.csv' , encoding='utf8')['item_id'].head(y_pred.shape[0])\n",
    "df_y_pred = pd.DataFrame(y_pred, columns = ['deal_probability'])\n",
    "df_resultado = pd.concat([df_test_item_id,df_y_pred],axis =1)\n",
    "df_resultado.to_csv('submission.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
